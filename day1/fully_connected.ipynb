{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Basic Feed Forward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow provides a convenient interface for MNIST data. This makes it really easy to test your code on a dataset that is commonly used. The code below shows you how to read MNIST images and store the labels as one-hot vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "MNIST = input_data.read_data_sets(\"../data/mnist\", one_hot = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create placeholders for X and Y. \n",
    "* Note that each MNIST image is 28x28. Additionally, the data will already be flattened into a 784 dimensional vector when we input it into the model\n",
    "* Each label is 10d - a vector element for every possible digit.\n",
    "* Make sure the shapes of the placeholders are defined so a variable number of images and labels can be fed in each batch. *This is what index 0 manages. Just put None instead of a dimension in this piece of the net*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create placeholders for X and Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = #__YOUR CODE HERE__\n",
    "Y = #__YOUR CODE HERE__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare each layer in the network and the final logits by:\n",
    "* Creating variables for weights and biases of the appropriate sizes\n",
    "* Applying ReLu on $X \\cdot W + b$\n",
    "\n",
    "This formulation is nearly the same as the logistic regression setup - except instead of applying softmax on the output of the hidden layers, we apply [relu](https://en.wikipedia.org/wiki/Rectifier_(neural_networks))\n",
    "\n",
    "Network Configurations:\n",
    "* First layer has 784 input features and 256 output features\n",
    "* Second layer has 256 input features and 256 output features\n",
    "* Third layer has 256 input features and 10 output features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W1 = tf.Variable(\"\"\"__YOUR CODE HERE__\"\"\")\n",
    "b1 = tf.Variable(\"\"\"__YOUR CODE HERE__\"\"\")\n",
    "layer1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = \"\"\"__YOUR CODE HERE__\"\"\"\n",
    "b2 = \"\"\"__YOUR CODE HERE__\"\"\"\n",
    "layer2 = tf.nn.relu(tf.matmul(\"\"\"__YOUR CODE HERE__\"\"\") + \"\"\"__YOUR CODE HERE__\"\"\")\n",
    "\n",
    "W_out = \"\"\"__YOUR CODE HERE__\"\"\"\n",
    "b_out = \"\"\"__YOUR CODE HERE__\"\"\"\n",
    "logits = \"\"\"__YOUR CODE HERE__\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the entropy using tf.nn.softmax_cross_entropy_with_logits. This will apply the softmax function to the logits before calculating the entropy. The loss as the mean over the entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entropy = tf.nn.softmax_cross_entropy_with_logits(logits=\"\"\"__YOUR CODE HERE__\"\"\", labels=\"\"\"__YOUR CODE HERE__\"\"\")\n",
    "loss = tf.reduce_mean(\"\"\"__YOUR CODE HERE__\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare the optimizer as the GradientDescentOptimizer with an appropriate learning rate. Set it to minimize the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = \"\"\"__YOUR CODE HERE__\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the accuracy by:\n",
    "* using tf.equal on the predicted label and the true label\n",
    "* casting that to a float and computing the mean over all examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_pred = tf.nn.softmax(logits)\n",
    "y_pred_cls = \"\"\"__YOUR CODE HERE__\"\"\"\n",
    "y_cls = \"\"\"__YOUR CODE HERE__\"\"\"\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(\"\"\"__YOUR CODE HERE__\"\"\"), tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start an Interactive Session and initialize all the global variables.\n",
    "* For each epoch, run the optimizer on each X,y batch and sum up the loss over all data points\n",
    "* Print the loss after each epoch\n",
    "\n",
    "We set the batch size to 128 and epochs to 25. Feel free to play around with these variables. Additionally, every 5 epochs we calculate validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 25\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(\"\"\"__YOUR CODE HERE__\"\"\")\n",
    "\n",
    "n_batches = (int) (MNIST.train.num_examples / batch_size)\n",
    "for i in range(epochs):\n",
    "    total_loss = 0\n",
    "    for batch in range(n_batches):\n",
    "        X_batch, Y_batch = MNIST.train.next_batch(batch_size)\n",
    "        o, l = \"\"\"__YOUR CODE HERE__\"\"\"\n",
    "        total_loss += l\n",
    "    print(\"Epoch {0}: {1}\".format(i, total_loss))\n",
    "    if i % 5 == 0 and i!= 0:\n",
    "        X_val, Y_val = MNIST.validation.next_batch(MNIST.validation.num_examples)\n",
    "        val_accuracy = \"\"\"__YOUR CODE HERE__\"\"\"\n",
    "        print(\"\\tVal Accuracy {0}\".format(val_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training and all validation, you'll want to return your test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Computing accuracy ...\")\n",
    "X_batch, y_batch = MNIST.test.next_batch(MNIST.test.num_examples)\n",
    "final_accuracy = sess.run(\"\"\"__YOUR CODE HERE__\"\"\")\n",
    "\n",
    "print (\"Test Accuracy {0}\".format(final_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
