{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Assorted examples, code doesn't actually run\n",
    "import tensorflow as tf\n",
    "\n",
    "### Example using TFRecords\n",
    "filenames = [\"/var/data/file1.tfrecord\", \"/var/data/file2.tfrecord\"]\n",
    "dataset = tf.data.TFRecordDataset(filenames) # create dataset from tfrecord files\n",
    "dataset = dataset.map(parser_fn) # parse tf record files into tensors\n",
    "dataset = dataset.shuffle(buffer_size=10000) # randomly sample 10000 from dataset\n",
    "dataset = dataset.batch(32) # batch tensors into batch size of 32\n",
    "dataset = dataset.repeat() # repeat dataset indefinitely\n",
    "\n",
    "### Example using python generator\n",
    "filenames = [..., ...]\n",
    "def gen():\n",
    "    for fname in filenames:\n",
    "        inputs, outputs = read_file(fname) # load numpy arrays from file\n",
    "        inputs, outputs = preprocess(inputs, outputs) # preprocess your inputs and outputs\n",
    "        yield inputs, outputs # yield inputs and outputs\n",
    "        \n",
    "dataset = tf.data.Dataset.from_generator(gen, output_types=(tf.float32, tf.float32), \n",
    "                                 output_shapes=(tf.TensorShape([None]), tf.TensorShape([None]))) \\\n",
    "                 .shard(num_workers, worker_index) \\\n",
    "                 .skip(5) \\\n",
    "                 .take(100) \\\n",
    "                 .filter(lambda x: True) \\\n",
    "                 .shuffle(num_examples_per_epoch * num_epochs) \\\n",
    "                 .batch(32) \\\n",
    "                 .map(map_func) # very useful if you need to run per batch preprocessing\n",
    "\n",
    "\n",
    "### Using datasets\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next() # tensor corresponding to output of dataset\n",
    "sess.run(next_element) # get one element from dataset\n",
    "\n",
    "training_op = model(next_element) # define some tensorflow graph on the inputs\n",
    "sess.run(training_op) # will pull the next_element from the dataset and then run it throught the defined tf graph\n",
    "\n",
    "\n",
    "### an alternative kind of iterator\n",
    "max_value = tf.placeholder(tf.int64, shape=[])\n",
    "dataset = tf.data.Dataset.range(max_value) # create a dataset of 1, 2, 3, ..., max_value\n",
    "iterator = dataset.make_intializable_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "sess.run(iterator.initializer, feed_dict={max_value: 5})\n",
    "for _ in range(5):\n",
    "    sess.run(next_element) # gets 1, 2, 3, 4, 5 in order\n",
    "\n",
    "sess.run(iterator.initializer, feed_dict={max_value: 200})\n",
    "for _ in range(200):\n",
    "    sess.run(next_element) # gets 1, 2, 3 ..., 200 in order\n",
    "    \n",
    "### ^ this kind of iterator is very useful for doing train/val splits and other such things"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data from here: http://vision.stanford.edu/aditya86/ImageNetDogs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Flatten\n",
    "from keras.applications import VGG16\n",
    "from keras.objectives import categorical_crossentropy\n",
    "import tensorflow as tf\n",
    "\n",
    "def model(inputs, targets, num_classes):\n",
    "    vgg_out = VGG16(weights='imagenet', include_top=False)(inputs)\n",
    "    vgg_out_flat = Flatten()(vgg_out)\n",
    "    fc_1 = Dense(256, activation='relu')(vgg_out_flat)\n",
    "    softmax = Dense(num_classes, activation='softmax')(fc_1)\n",
    "    \n",
    "    loss = tf.reduce_mean(categorical_crossentropy(targets, softmax))\n",
    "    \n",
    "    training_op = tf.train.AdamOptimizer().minimize(loss)\n",
    "    return training_op, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "filenames_with_labels = []\n",
    "\n",
    "for path in os.listdir('Images'):\n",
    "    m = re.match(\".*-(.*)\", path)\n",
    "    if m is None:\n",
    "        continue\n",
    "    label = m.group(1)\n",
    "    for fname in os.listdir(os.path.join('Images', path)):\n",
    "        filenames_with_labels.append((os.path.join('Images', path, fname), label))\n",
    "random.shuffle(filenames_with_labels)\n",
    "unique_labels =  np.unique([x[1] for x in filenames_with_labels])\n",
    "label_to_ind = {label: i for i, label in enumerate(unique_labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "def generator():\n",
    "    for image_fname, label in filenames_with_labels:\n",
    "        img = image.load_img(image_fname, target_size=(224, 224))\n",
    "        img = image.img_to_array(img)\n",
    "        img = preprocess_input(img)\n",
    "        \n",
    "        label_ind = label_to_ind[label]\n",
    "        one_hot = np.eye(len(unique_labels))[label_ind]\n",
    "        \n",
    "        yield {'image': img, 'target': one_hot}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "batch_size = 2\n",
    "output_types = {'image': tf.float32,\n",
    "                'target': tf.float32}\n",
    "output_shapes = {'image': tf.TensorShape([224, 224, 3]),\n",
    "                 'target': tf.TensorShape([len(unique_labels)])}\n",
    "def batch_process_map(tensor_dict):\n",
    "    d = tensor_dict.copy()\n",
    "    d['mean'] = tf.reduce_mean(tensor_dict['image'], axis=[1,2,3])\n",
    "    return d\n",
    "ds = tf.data.Dataset.from_generator(generator, output_types, output_shapes) \\\n",
    "                    .shuffle(100) \\\n",
    "                    .batch(batch_size)\n",
    "                    .map(batch_process_map)\n",
    "iterator = ds.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()\n",
    "training_op, loss = model(next_element['image'], next_element['target'], len(unique_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "with tf.Session() as sess:\n",
    "    step = 0\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    while True:\n",
    "        try:\n",
    "            _, l = sess.run([training_op, loss])\n",
    "            print(\"Loss for step {}: {}\".format(step, l))\n",
    "            if step % len(filenames_with_labels) == 0:\n",
    "                print(\"Starting epoch: {}\".format(step / len(filenames_with_labels)))\n",
    "            step += 1\n",
    "            sys.stdout.flush()\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print(\"Finished training\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Flatten\n",
    "from keras.applications import VGG16\n",
    "from keras.objectives import categorical_crossentropy\n",
    "import tensorflow as tf\n",
    "\n",
    "def model(inputs, targets, num_classes):\n",
    "    tf.summary.image(\"images\", inputs)\n",
    "    vgg_out = VGG16(weights='imagenet', include_top=False)(inputs)\n",
    "    vgg_out_flat = Flatten()(vgg_out)\n",
    "    fc_1 = Dense(256, activation='relu')(vgg_out_flat)\n",
    "    tf.summary.histogram(\"fc_1\", fc_1)\n",
    "    softmax = Dense(num_classes, activation='softmax')(fc_1)\n",
    "    tf.summary.histogram(\"activations\", softmax)\n",
    "    \n",
    "    loss = tf.reduce_mean(categorical_crossentropy(targets, softmax))\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "    \n",
    "    training_op = tf.train.AdamOptimizer().minimize(loss)\n",
    "    return training_op, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "output_types = {'image': tf.float32,\n",
    "                'target': tf.float32}\n",
    "output_shapes = {'image': tf.TensorShape([224, 224, 3]),\n",
    "                 'target': tf.TensorShape([len(unique_labels)])}\n",
    "ds = tf.data.Dataset.from_generator(generator, output_types, output_shapes) \\\n",
    "                    .shuffle(100) \\\n",
    "                    .batch(batch_size)\n",
    "iterator = ds.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()\n",
    "training_op, loss = model(next_element['image'], next_element['target'], len(unique_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "with tf.Session() as sess:\n",
    "    file_writer = tf.summary.FileWriter('logs', sess.graph)\n",
    "    step = 0\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    while True:\n",
    "        try:\n",
    "            merged = tf.summary.merge_all()\n",
    "            summary, _, l = sess.run([merged, training_op, loss])\n",
    "            file_writer.add_summary(summary, step)\n",
    "            print(\"Loss for step {}: {}\".format(step, l))\n",
    "            if step % len(filenames_with_labels) == 0:\n",
    "                print(\"Starting epoch: {}\".format(step / len(filenames_with_labels)))\n",
    "            step += 1\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print(\"Finished training\")\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
