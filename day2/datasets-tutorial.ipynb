{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Assorted examples, code doesn't actually run\n",
    "import tensorflow as tf\n",
    "\n",
    "### Example using TFRecords\n",
    "filenames = [\"/var/data/file1.tfrecord\", \"/var/data/file2.tfrecord\"]\n",
    "dataset = tf.data.TFRecordDataset(filenames) # create dataset from tfrecord files\n",
    "dataset = dataset.map(parser_fn) # parse tf record files into tensors\n",
    "dataset = dataset.shuffle(buffer_size=10000) # randomly sample 10000 from dataset\n",
    "dataset = dataset.batch(32) # batch tensors into batch size of 32\n",
    "dataset = dataset.repeat() # repeat dataset indefinitely\n",
    "\n",
    "### Example using python generator\n",
    "filenames = [..., ...]\n",
    "def gen():\n",
    "    for fname in filenames:\n",
    "        inputs, outputs = read_file(fname) # load numpy arrays from file\n",
    "        inputs, outputs = preprocess(inputs, outputs) # preprocess your inputs and outputs\n",
    "        yield inputs, outputs # yield inputs and outputs\n",
    "        \n",
    "dataset = tf.data.Dataset.from_generator(gen, output_types=(tf.float32, tf.float32), \n",
    "                                 output_shapes=(tf.TensorShape([None]), tf.TensorShape([None]))) \\\n",
    "                 .shard(num_workers, worker_index) \\\n",
    "                 .skip(5) \\\n",
    "                 .take(100) \\\n",
    "                 .filter(lambda x: True) \\\n",
    "                 .shuffle(num_examples_per_epoch * num_epochs) \\\n",
    "                 .batch(32) \\\n",
    "                 .map(map_func) # very useful if you need to run per batch preprocessing\n",
    "\n",
    "\n",
    "### Using datasets\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next() # tensor corresponding to output of dataset\n",
    "sess.run(next_element) # get one element from dataset\n",
    "\n",
    "training_op = model(next_element) # define some tensorflow graph on the inputs\n",
    "sess.run(training_op) # will pull the next_element from the dataset and then run it throught the defined tf graph\n",
    "\n",
    "\n",
    "### an alternative kind of iterator\n",
    "max_value = tf.placeholder(tf.int64, shape=[])\n",
    "dataset = tf.data.Dataset.range(max_value) # create a dataset of 1, 2, 3, ..., max_value\n",
    "iterator = dataset.make_intializable_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "sess.run(iterator.initializer, feed_dict={max_value: 5})\n",
    "for _ in range(5):\n",
    "    sess.run(next_element) # gets 1, 2, 3, 4, 5 in order\n",
    "\n",
    "sess.run(iterator.initializer, feed_dict={max_value: 200})\n",
    "for _ in range(200):\n",
    "    sess.run(next_element) # gets 1, 2, 3 ..., 200 in order\n",
    "    \n",
    "### ^ this kind of iterator is very useful for doing train/val splits and other such things"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data from here: http://vision.stanford.edu/aditya86/ImageNetDogs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Flatten\n",
    "from keras.applications import VGG16\n",
    "from keras.objectives import categorical_crossentropy\n",
    "import tensorflow as tf\n",
    "\n",
    "def model(inputs, targets, num_classes):\n",
    "    ### write a model which has the following 'layers'\n",
    "    # VGG16\n",
    "    # FC 256 neurons\n",
    "    # softmax with num_classes neurons\n",
    "    \n",
    "    vgg_out = VGG16(weights='imagenet', include_top=False)(inputs)\n",
    "    ###YOUR CODE HERE\n",
    "    \n",
    "    # calculate scalar loss based on targets and output of net\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    training_op = tf.train.AdamOptimizer().minimize(loss)\n",
    "    return training_op, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "filenames_with_labels = []\n",
    "\n",
    "for path in os.listdir('Images'):\n",
    "    m = re.match(\".*-(.*)\", path)\n",
    "    if m is None:\n",
    "        continue\n",
    "    label = m.group(1)\n",
    "    for fname in os.listdir(os.path.join('Images', path)):\n",
    "        filenames_with_labels.append((os.path.join('Images', path, fname), label))\n",
    "unique_labels =  np.unique([x[1] for x in filenames_with_labels])\n",
    "label_to_ind = {label: i for i, label in enumerate(unique_labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "def generator():\n",
    "    for image_fname, label in filenames_with_labels:\n",
    "        ### load image from file into shape (224, 224, 3)\n",
    "        \n",
    "        ### rescale image to -1 to 1\n",
    "        ### hint look at the imports above for useful tools\n",
    "        \n",
    "        # create one hot vector from label\n",
    "        \n",
    "        yield {'image': img, 'target': one_hot}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "batch_size = 2\n",
    "output_types = ??? #hint: should be a dictionary\n",
    "output_shapes = ??? # hint: should be a dictionary\n",
    "dataset = ## generate dataset from generator, then shuffle and batch\n",
    "\n",
    "iterator = # make an iterator from the dataset\n",
    "next_element = #get the next dictionary of tensors from the iterator\n",
    "training_op, loss = model(next_element['image'], next_element['target'], len(unique_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "with tf.Session() as sess:\n",
    "    step = 0\n",
    "    ### initialize tf global variables here\n",
    "    while True:\n",
    "        try:\n",
    "            _, l = sess.run([training_op, loss])\n",
    "            print(\"Loss for step {}: {}\".format(step, l))\n",
    "            if step % len(filenames_with_labels) == 0:\n",
    "                print(\"Starting epoch: {}\".format(step / len(filenames_with_labels)))\n",
    "            step += 1\n",
    "            sys.stdout.flush()\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print(\"Finished training\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Flatten\n",
    "from keras.applications import VGG16\n",
    "from keras.objectives import categorical_crossentropy\n",
    "import tensorflow as tf\n",
    "\n",
    "def model(inputs, targets, num_classes):\n",
    "    ### Copy code from above model and then add a histogram for fc_1\n",
    "    # a histogram for softmax called activations and\n",
    "    # a scalar summary for the loss and\n",
    "    # an image summary for inputs\n",
    "    vgg_out = VGG16(weights='imagenet', include_top=False)(inputs)\n",
    "\n",
    "    \n",
    "    training_op = tf.train.AdamOptimizer().minimize(loss)\n",
    "    return training_op, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## HINT: this stuff is all the same as before\n",
    "num_epochs = 1\n",
    "batch_size = 2\n",
    "output_types = ??? #hint: should be a dictionary\n",
    "output_shapes = ??? # hint: should be a dictionary\n",
    "dataset = ## generate dataset from generator, then shuffle and batch\n",
    "\n",
    "iterator = # make an iterator from the dataset\n",
    "next_element = #get the next dictionary of tensors from the iterator\n",
    "training_op, loss = model(next_element['image'], next_element['target'], len(unique_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "with tf.Session() as sess:\n",
    "    # create a file writer to write summaries\n",
    "    step = 0\n",
    "    # initialize tf global variables as before\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            merged = # create an op to merge all summaries into one op\n",
    "            summary, _, l = sess.run([merged, training_op, loss])\n",
    "            \n",
    "            file_writer.??? # add summary result to file writer\n",
    "            print(\"Loss for step {}: {}\".format(step, l))\n",
    "            if step % len(filenames_with_labels) == 0:\n",
    "                print(\"Starting epoch: {}\".format(step / len(filenames_with_labels)))\n",
    "            step += 1\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print(\"Finished training\")\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
